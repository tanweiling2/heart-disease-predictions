,,,,,,,,,
DECISION TREE,,,,,,,,,
Scoring metric ,Test set performance,,,,,Scoring Metric Performance ,,Difference in scoring metric performance ,Notes
,AUC,Precision for y=1,Recall for y =1,Accuracy,Test Log-loss,Train set ,Test Set,,
roc-auc,0.837,0.14,0.78,78.00%,0.522,0.850,0.837,0.013,
neg_log_loss,0.819,0.10,0.90,66.00%,0.534,0.475,0.534,-0.059,"model performed better in train, (lower log loss) overfiited)"
f1-score,0.834,0.15,0.76,80.00%,0.511,0.259,0.247,0.012,
recall,0.745,0.08,0.98,53.00%,0.538,0.958,0.976,-0.018,recall is high but FP is very high (v low precision)
f2-score,0.834,0.15,0.76,80.00%,0.511,0.427,0.414,0.013,
,,,,,,,,,
RANDOM FOREST,,,,,,,,,
Scoring metric,Test set performance,,,,,Scoring Metric Performance ,,Difference in scoring metric performance ,
,AUC ,Precision for y=1,Recall for y =1,Accuracy,Test Log-loss,Train set ,Test Set,,
roc-auc,0.855,0.14,0.73,79.58%,0.401,0.908,0.855,-0.053,
neg_log_loss,0.867,0.17,0.10,94.90%,0.176,0.114,0.176,0.062,model overfitted and could not predict +ve outcomes at all.
f1-score,0.859,0.20,0.66,86.98%,0.300,0.424,0.302,-0.122,
recall,0.832,0.11,0.85,70.10%,0.560,0.783,0.854,0.071,performs better during testing! not overfitting
f2-score,0.848,0.14,0.73,79.48%,0.410,0.462,0.395,-0.068,
,,,,,,,,,
GRADIENT BOOSTING,,,,,,,,,
,Test set performance,,,,,Scoring Metric Performance ,,Difference in scoring metric performance ,
Scoring metric,AUC ,Precision for y=1,Recall for y =1,Accuracy (%),Test Log-loss,Train set ,Test Set,,
roc-auc,0.843,0.12,0.80,74.48%,0.464,0.886,0.842,-0.044,
neg_log_loss,0.818,0.18,0.15,93.54%,0.173,0.030,0.173,0.143,
f1-score,0.846,0.14,0.76,79.06%,0.541,0.256,0.236,-0.020,*all three scoring metrics achieved same results & same confusion matrix
recall,0.846,0.14,0.76,79.06%,0.541,0.873,0.805,-0.069,
f2-score,0.846,0.14,0.76,79.06%,0.541,0.428,0.402,-0.026,
,,,,,,,,,
XGBOOST,,,,,,,,,
,Test set performance,,,,,Scoring Metric Performance ,,Difference in scoring metric performance ,
Scoring metric,AUC,Precision for y=1,Recall for y =1,Accuracy,Test Log-loss,Train set ,Test Set,,
roc-auc,0.853,0.13,0.83,75.52%,0.461,0.882,0.853,-0.029,lower FP for the outcome of y=0 => slightly higher precision
neg_log_loss,0.812,0.10,0.05,93.96%,0.175,0.033,0.175,0.141,
f1-score,0.838,0.19,0.59,87.40%,0.496,0.479,0.284,-0.195,
recall,0.853,0.13,0.83,75.52%,0.461,0.861,0.829,-0.032,* exact same result as roc-auc in terms of y =1
f2-score,0.847,0.13,0.78,76.35%,0.465,0.440,0.386,-0.053,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
TREE BASED MODELS,,,,,,,,,
,Test set performance,,,,,Scoring Metric Performance ,,Difference in scoring metric performance ,
Scoring metric,AUC ,Precision for y=1,Recall for y =1,Accuracy,Test Log-loss,Train set ,Test Set,,
DT-roc-auc,0.837,0.14,0.78,78.00%,0.522,0.850,0.837,0.013,
RF-recall,0.832,0.11,0.85,70.10%,0.560,0.783,0.854,0.071,
GB-roc-auc,0.843,0.12,0.80,74.48%,0.464,0.886,0.842,-0.044,
XGB-roc_auc,0.853,0.13,0.83,75.52%,0.461,0.882,0.853,-0.029,
,,,,,,,,,
RF with recall scoring metric performed the best for the TBM. ,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
(Classification Model) - {Scoring_metric},DT-f1-score,RF-roc_auc,GB-roc_auc,XGB-recall,,,,,
AUC ,0.837,0.832,0.843,0.853,,,,,
Precision for y=1,0.14,0.11,0.12,0.13,,,,,
Recall for y =1,0.78,0.85,0.80,0.83,,,,,
Accuracy,78.00%,70.10%,74.48%,75.52%,,,,,
Test Log-loss,0.522,0.560,0.464,0.461,,,,,